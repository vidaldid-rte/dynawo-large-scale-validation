#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# (c) Grupo AIA
#     gaitanv@aia.es
#     marinjl@aia.es
#
# calc_curve_diffmetrics.py:
#
# Given a directory containing processed cases, of type EITHER Astre vs. Dynawo OR
# Dynawo vs. Dynawo, and all of them derived from a common BASECASE, this script
# extracts several "reduced parameters" that characterize the curve signals. It works
# on the curve files produced by Astre and Dynawo, where all variables (and their
# order) are expected to be suitably prepared in order to have exactly the same names.
#
#   * On input: you have to provide the directory that contains the curve files (e.g.
#     "*-AstreCurves.csv.xz", etc.), a filename prefix for them (e.g. "shunt_"), and the
#     common BASECASE from which the cases were derived.
#
#   * On output: a file "crv_reducedparams.csv" containing dSS, dPP, etc. for all
#     variables, and for all "case A" and "case B" files (whether they are
#     Astre-vs-Dynawo or Dynawo-vs-Dynawo).
#

import sys
import os
import glob
from pathlib import Path
from collections import namedtuple
import pandas as pd
import numpy as np
from scipy.interpolate import interp1d

# Relative imports only work for proper Python packages, but we do not want to
# structure all these as a package; we'd like to keep them as a collection of loose
# Python scripts, at least for now (because this is not really a Python library). So
# the following hack is ugly, but needed:
sys.path.insert(1, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
# Alternatively, you could set PYTHONPATH to PYTHONPATH="/<dir>/dynawo-validation-AIA"
from dynawo_validation.dynawaltz.pipeline.dwo_jobinfo import (
    is_astdwo,
    is_dwodwo,
    get_dwo_tparams,
    get_dwodwo_tparams,
)  # noqa: E402


REL_TOL = 1.0e-5  # when testing for the SS, relative tolerance in signal
T_TOL = 1.0e-2  # when comparing time instants for equality, absolute tolerance
STABILITY_MINTIME = 60  # require at least these seconds for the SS to be achieved
TT_MIN_FOR_PRONY = 60  # transient must be at least these seconds long to do Prony
PRONY_ORDER = 7  # number of damped sinusoid components
PRONY_SAMPLES = 100  # number of (interpolated) data points used for Prony analysis
AST_SUFFIX = "-AstreCurves.csv.xz"
DWO_SUFFIX = "-DynawoCurves.csv.xz"
DWO_SUFFIX_A = "-DynawoCurvesA.csv.xz"
DWO_SUFFIX_B = "-DynawoCurvesB.csv.xz"
verbose = True


def main():
    if len(sys.argv) != 4:
        print("\nUsage: %s CRV_DIR PREFIX BASECASE\n" % sys.argv[0])
        return 2
    crv_dir = sys.argv[1]
    prefix = sys.argv[2]
    base_case = sys.argv[3]

    # Get common time params from the BASECASE
    if is_astdwo(base_case):
        case_type = "astdwo"
        dwo_tparams = get_dwo_tparams(base_case)
        startTime = dwo_tparams.startTime
        tEvent = dwo_tparams.event_tEvent
    elif is_dwodwo(base_case):
        case_type = "dwodwo"
        dwo_tparamsA, _ = get_dwodwo_tparams(base_case)
        startTime = dwo_tparamsA.startTime
        tEvent = dwo_tparamsA.event_tEvent
    else:
        raise ValueError("Case %s is neither an ast-dwo nor a dwo-dwo case" % base_case)

    print(
        "Calculating diffmetrics for curves in: %s (CASE_TYPE=%s)"
        % (crv_dir, case_type)
    )

    # Get the list of curve files to process
    file_list = list_inputfiles(case_type, crv_dir, prefix)

    # Calculate all diffmetrics and output the results to file
    process_all_curves(case_type, crv_dir, file_list, startTime, tEvent)

    return 0

def find_launchers(pathtofiles):
    launcherA = None
    launcherB = None
    for file in os.listdir(pathtofiles):
        basefile = os.path.basename(file)
        if ".LAUNCHER_A_WAS_" == basefile[:16] and launcherA == None:
            launcherA = basefile[16:]
        elif ".LAUNCHER_A_WAS_" == basefile[:16]:  
            raise ValueError(f"Two or more .LAUNCHER_WAS_A in results dir")
        elif ".LAUNCHER_B_WAS_" == basefile[:16] and launcherB == None:
            launcherB = basefile[16:]
        elif ".LAUNCHER_B_WAS_" == basefile[:16]:     
            raise ValueError(f"Two or more .LAUNCHER_WAS_A in results dir")    
    return launcherA, launcherB    
    
def list_inputfiles(case_type, crv_dir, prefix):
    if not os.path.isdir(crv_dir):
        raise ValueError("input directory %s not found" % crv_dir)
    launcherA, launcherB = find_launchers(crv_dir+'/../../')    
    if case_type == "astdwo":
        if launcherA[:5] == 'astre':
            caseA_suffix = AST_SUFFIX
            caseB_suffix = DWO_SUFFIX
        else:
            caseA_suffix = DWO_SUFFIX 
            caseB_suffix = AST_SUFFIX
    elif case_type == "dwodwo":
        caseA_suffix = DWO_SUFFIX_A
        caseB_suffix = DWO_SUFFIX_B
    else:
        raise ValueError("case_type is neither 'astdwo' nor 'dwodwo'")

    # First get the list of all "case A" files
    caseA_filepattern = crv_dir + "/" + prefix + "*" + caseA_suffix
    caseA_files = glob.glob(caseA_filepattern)
    if len(caseA_files) == 0:
        raise ValueError("no 'case A' input files found with prefix %s\n" % prefix)

    # Then find their corresponding "case B" counterparts
    Crv_Pair = namedtuple("Crv_Pair", ["caseA", "caseB"])
    file_list = dict()
    for caseA_file in caseA_files:
        case_label = caseA_file.split(caseA_suffix)[0]
        case_label = [e+prefix for e in case_label.split(prefix) if e][1:]
        case_label[-1] = case_label[-1].replace(prefix,"")
        case_label = ''.join(case_label)
        caseB_file = caseA_file.split(caseA_suffix)[0] + caseB_suffix
        if not (os.path.isfile(caseB_file)):
            raise ValueError("'case B' crv file %s not found\n" % caseB_file)
        file_list[case_label] = Crv_Pair(caseA=caseA_file, caseB=caseB_file)

    if verbose:
        print("crv_dir: %s" % crv_dir)
        print("prefix: %s" % prefix)
        print("List of cases to process: (total: %d)" % len(file_list))
        case_list = sorted(file_list.keys())
        if len(case_list) < 10:
            print(case_list)
        else:
            print(case_list[:5] + ["..."] + case_list[-5:])
        print()

    return file_list


def process_all_curves(case_type, crv_dir, file_list, start_time, t_event):
    all_casesA = pd.DataFrame()
    all_casesB = pd.DataFrame()
    cnames = ["dSS", "dPP", "TT", "period", "damping", "is_preStab", "is_postStab"]
    t0_event = t_event - start_time  # adjust time offset (Dynawo cases)

    print("Processing ", end="")
    for case_label in file_list:
        crv_A = pd.read_csv(file_list[case_label].caseA, sep=";", compression="infer")
        crv_B = pd.read_csv(file_list[case_label].caseB, sep=";", compression="infer")

        # Clean Dynawo's extra ";" at end-of-lines
        if case_type == "astdwo":
            launcherA, launcherB = find_launchers(crv_dir+'/../../')
            if launcherA[:5] == 'astre':
                crv_B = crv_B.iloc[:, :-1]
            else:
                crv_A = crv_A.iloc[:, :-1]
        else:
            crv_A = crv_A.iloc[:, :-1]
            crv_B = crv_B.iloc[:, :-1]

        # Check vars. They should match by order AND name
        if list(crv_A.columns) != list(crv_B.columns):
            raise ValueError(
                "'case A' and 'case B' curves differ in the name or number of fields"
                " (case %s)\n" % case_label
            )

        # Check that Dynawo's simulation startTime is consistently the same as in the
        # BASECASE, and adjust the time offset w.r.t. Astre, which is always zero
        if case_type == "dwodwo":
            if abs(float(crv_A["time"].iloc[0]) - float(start_time)) > T_TOL:
                raise ValueError(
                    "The startTime in DynawoA curve file (case %s) differs from"
                    " the one in the BASECASE!\n" % case_label
                )
            crv_A["time"] = crv_A["time"] - start_time
        
       if abs(float(crv_B["time"].iloc[0]) - float(start_time)) > T_TOL:
            raise ValueError(
                "The startTime in DynawoB curve file (case %s) differs from"
                " the one in the BASECASE!\n" % case_label
            )
        crv_B["time"] = crv_B["time"] - start_time

        # Warn about simulations that stopped before they were supposed to
        if abs(float(crv_A["time"].iloc[-1]) - float(crv_B["time"].iloc[-1])) > T_TOL:
            is_crv_time_matching = False
            print(
                "   WARNING: 'case A' and 'case B' curves stop at different times"
                " (case %s)\n" % case_label
            )
        else:
            is_crv_time_matching = True

        # Process all variables for this case
        var_list = list(crv_A.columns)[1:]
        resultsA = [extract_crv_reduced_params(crv_A, x, t0_event) for x in var_list]
        resultsB = [extract_crv_reduced_params(crv_B, x, t0_event) for x in var_list]

        # Structure the results in a dataframe
        df_resultsA = pd.DataFrame(data=resultsA, columns=cnames)
        df_resultsB = pd.DataFrame(data=resultsB, columns=cnames)
        df_resultsA["dev"] = case_label
        df_resultsB["dev"] = case_label
        df_resultsA["vars"] = var_list
        df_resultsB["vars"] = var_list
        df_resultsB["is_crv_time_matching"] = is_crv_time_matching

        # Collect results for all cases
        all_casesA = all_casesA.append(df_resultsA)
        all_casesB = all_casesB.append(df_resultsB)
        print(".", end="", flush=True)

    print(" OK.")

    # Group all reduced signal parameters in one single dataframe
    # TODO: rename these fields as A and B (and resp. in the Notebook, too)
    reduced_params = all_casesA[["dev", "vars"]].copy(deep=True)
    reduced_params["dSS_ast"] = all_casesA.dSS
    reduced_params["dSS_dwo"] = all_casesB.dSS
    reduced_params["dPP_ast"] = all_casesA.dPP
    reduced_params["dPP_dwo"] = all_casesB.dPP
    reduced_params["TT_ast"] = all_casesA.TT
    reduced_params["TT_dwo"] = all_casesB.TT
    reduced_params["period_ast"] = all_casesA.period
    reduced_params["period_dwo"] = all_casesB.period
    reduced_params["damp_ast"] = all_casesA.damping
    reduced_params["damp_dwo"] = all_casesB.damping
    reduced_params["is_preStab_ast"] = all_casesA.is_preStab
    reduced_params["is_preStab_dwo"] = all_casesB.is_preStab
    reduced_params["is_postStab_ast"] = all_casesA.is_postStab
    reduced_params["is_postStab_dwo"] = all_casesB.is_postStab
    reduced_params["is_crv_time_matching"] = all_casesB.is_crv_time_matching

    # Output to file
    metrics_dir = crv_dir + "/../metrics"
    Path(metrics_dir).mkdir(parents=False, exist_ok=True)
    reduced_params.to_csv(
        metrics_dir + "/crv_reducedparams.csv",
        sep=";",
        index=False,
        float_format="%.6f",
    )
    print("Saved reduced parameters for curve data under: %s" % metrics_dir)

    # Record any bad cases
    bad_cases = (
        reduced_params["dev"].loc[~reduced_params["is_crv_time_matching"]].unique()
    )
    np.savetxt(
        metrics_dir + "/bad_cases.csv",
        bad_cases,
        fmt="%s",
        header="Cases where the simulation time span doesn't match "
        "(probably because of integration error)",
    )
    print("Note: simulations that stopped early will be listed in 'bad_cases.csv'")


#################################################################################
# Extraction of the relevant reduced parameters for the curve (dSS, dPP, etc.)
#################################################################################
def extract_crv_reduced_params(df, var, t_event):
    # Summary:
    #   * Initial Steady State value: taken from the instant right before the event
    #   * Final Steady State value: taken from the very last instant
    #   * With these, we calculate dSS and dPP
    #   * Transient end: defined as the time at which all subsequent values are within
    #     some (relative) tolerance of the final Steady State value.
    #   * Tag whether the case is pre-contingency & post-contingency stable
    #   * Window the transient and do Prony analysis to calculate period & damping of
    #     the main component
    #
    # Stability (pre or post) is defined as "signal has not deviated from the last
    # value more than REL_TOL (relative error), for at least STABILITY_MINTIME seconds"
    #

    x = df[var].values
    t = df["time"].values

    # Net change in steady state (dSS):
    idx_SSpre = np.nonzero(t < t_event)[0][-1]
    idx_SSpost = -1
    dSS = x[idx_SSpost] - x[idx_SSpre]

    # Peak-to-peak amplitude (sPP):
    dPP = np.max(x[idx_SSpre:]) - np.min(x[idx_SSpre:])

    # Transient time (TT):
    tol = max(abs(x[idx_SSpost]) * REL_TOL, REL_TOL)
    # this assumes datapoints at least until t=t_event; otherwise it will give error:
    idx_transientEnd = np.nonzero((t >= t_event) & (np.abs(x - x[idx_SSpost]) < tol))[
        0
    ][0]
    TT = t[idx_transientEnd] - t_event

    # Post-contingency stability:
    if (t[-1] - t[idx_transientEnd]) >= STABILITY_MINTIME:
        is_postStab = True
    else:
        is_postStab = False

    # Pre-contingency stability:
    tol = max(abs(x[idx_SSpre]) * REL_TOL, REL_TOL)
    idx_preTransientEnd = np.nonzero((t < t_event) & (np.abs(x - x[idx_SSpre]) < tol))[
        0
    ][0]
    if (t_event - t[idx_preTransientEnd]) >= STABILITY_MINTIME:
        is_preStab = True
    else:
        is_preStab = False

    # Period and damping of the transient (via Prony analysis):
    # first, trim the signal to the transient window
    idxs_postEvent = np.nonzero(t > t_event)[0]
    if idxs_postEvent.size == 0:
        # no data: simulation bombed out right at t = t_event!
        return [dSS, dPP, TT, 0, 0, is_preStab, is_postStab]
    idx_transientStart = idxs_postEvent[0]
    if (
        TT < TT_MIN_FOR_PRONY
        or (idx_transientEnd - idx_transientStart) < 2 * PRONY_ORDER
    ):
        # transient too short for any meaningful Prony analysis
        return [dSS, dPP, TT, 0, 0, is_preStab, is_postStab]
    t_trans = t[idx_transientStart:idx_transientEnd]
    x_trans = x[idx_transientStart:idx_transientEnd]
    # then, smooth out the kinks
    t_trans, x_trans = avg_duplicate_points(t_trans, x_trans)
    # now, interpolate to get samples that are equally-spaced in time (because Dynawo's
    # time-step could be variable)
    f = interp1d(t_trans, x_trans, kind="cubic")
    t_interp = np.linspace(t_trans[0], t_trans[-1], num=PRONY_SAMPLES, endpoint=True)
    x_interp = f(t_interp)
    # and finally, do the Prony analysis
    sampling_rate = len(x_interp) / (t_interp[-1] - t_interp[0])
    x_interp = x_interp - np.mean(x_interp)
    period, damping = get_peri_damp(x_interp, sampling_rate)

    return [dSS, dPP, TT, period, damping, is_preStab, is_postStab]


# Both Astre and Dynawo may output two data points with the same timestamp. This
# function cleans the signal by taking the average of such cases.  Uses the elegant
# solution given by Gabriel S. GusmÃ£o (see https://stackoverflow.com/questions/7790611)
def avg_duplicate_points(t_orig, x_orig):
    t, ind, counts = np.unique(t_orig, return_index=True, return_counts=True)
    x = x_orig[ind]
    for dup in t[counts > 1]:
        x[t == dup] = np.average(x_orig[t_orig == dup])
    return t, x


#################################
# Prony analysis
#################################
def convm(x, p):
    """Generates a convolution matrix

    Usage: X = convm(x,p)
    Given a vector x of length N, an N+p-1 by p convolution matrix is
    generated of the following form:
              |  x(0)  0      0     ...      0    |
              |  x(1) x(0)    0     ...      0    |
              |  x(2) x(1)   x(0)   ...      0    |
         X =  |   .    .      .              .    |
              |   .    .      .              .    |
              |   .    .      .              .    |
              |  x(N) x(N-1) x(N-2) ...  x(N-p+1) |
              |   0   x(N)   x(N-1) ...  x(N-p+2) |
              |   .    .      .              .    |
              |   .    .      .              .    |
              |   0    0      0     ...    x(N)   |

    That is, x is assumed to be causal, and zero-valued after N.
    """
    N = len(x) + 2 * p - 2
    xpad = np.concatenate([np.zeros(p - 1), x[:], np.zeros(p - 1)])
    X = np.zeros((len(x) + p - 1, p))
    # Construct X column by column
    for i in range(p):
        X[:, i] = xpad[p - i - 1 : N - i]

    return X


def prony(x, p, q):
    """Model a signal using Prony's method

    Usage: [b,a,err] = prony(x,p,q)

    The input sequence x is modeled as the unit sample response of
    a filter having a system function of the form
        H(z) = B(z)/A(z)
    The polynomials B(z) and A(z) are formed from the vectors
        b=[b(0), b(1), ... b(q)]
        a=[1   , a(1), ... a(p)]
    The input q defines the number of zeros in the model
    and p defines the number of poles. The modeling error is
    returned in err.

    This comes from Hayes, p. 149, 153, etc

    """
    x = x[:]
    N = len(x)
    if p + q >= len(x):
        return [1, 0], [1, 0], 0

    # This formulation uses eq. 4.50, p. 153
    # Set up the convolution matrices
    X = convm(x, p + 1)
    Xq = X[q : N + p - 1, 0:p]
    xq1 = -X[q + 1 : N + p, 0]

    # Solve for denominator coefficients
    if p > 0:
        a = np.linalg.lstsq(Xq, xq1, rcond=None)[0]
        a = np.insert(a, 0, 1)  # a(0) is 1
    else:
        # all-zero model
        a = np.array(1)

    # Solve for the model error
    err = np.dot(x[q + 1 : N].conj().T, X[q + 1 : N, 0 : p + 1])
    err = np.dot(err, a)

    # Solve for numerator coefficients
    if q > 0:
        # (This is the same as for Pad?)
        b = np.dot(X[0 : q + 1, 0 : p + 1], a)
    else:
        # all-pole model
        # b(0) is x(0), but a better solution is to match energy
        b = np.sqrt(err)

    return b, a, err


def get_peri_damp(x, fsamp):
    p = PRONY_ORDER
    q = PRONY_ORDER - 1
    b, a, err = prony(x, p, q)
    with np.errstate(divide="ignore"):  # we may get some Inf here
        lamb = fsamp * np.log(np.roots(a))

    freq = np.abs(np.imag(lamb)) / (2 * np.pi)

    with np.errstate(invalid="ignore"):  # we may get some NaN here (0/0)
        damp = np.real(lamb) / np.abs(lamb)

    mask = np.argmin(freq)
    freq = freq[mask]
    damp = damp[mask]

    if freq == 0:
        peri = 0
    else:
        peri = 1 / freq

    return peri, damp


if __name__ == "__main__":
    sys.exit(main())
